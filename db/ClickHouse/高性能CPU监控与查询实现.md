### é«˜æ€§èƒ½CPUç›‘æ§ä¸æŸ¥è¯¢å®ç°ï¼ˆPython+chdb+psutilï¼‰

#### æ ¸å¿ƒè®¾è®¡æ€è·¯
1. **æ€§èƒ½æè‡´ä¼˜åŒ–**ï¼š
   - é‡‡ç”¨chdbï¼ˆåŸºäºClickHouseçš„åµŒå…¥å¼å¼•æ“ï¼‰å­˜å‚¨æ—¶åºæ•°æ®ï¼Œåˆ©ç”¨åˆ—å¼å­˜å‚¨å’Œé«˜æ•ˆå‹ç¼©æå‡å†™å…¥/æŸ¥è¯¢æ€§èƒ½
   - æ‰¹é‡å†™å…¥å‡å°‘IOå¼€é”€ï¼Œé¿å…å•æ¡å†™å…¥çš„æ€§èƒ½æŸè€—
   - å‡å°‘ä¸å¿…è¦çš„ç³»ç»Ÿè°ƒç”¨ï¼Œpsutilé‡‡é›†æ•°æ®æ—¶ä½¿ç”¨æœ€å°åŒ–å‚æ•°
   - æ–‡ä»¶å­˜å‚¨é‡‡ç”¨ClickHouseçš„åŸç”Ÿæ ¼å¼ï¼ˆParquetï¼‰ï¼Œå…¼é¡¾æ€§èƒ½å’Œå…¼å®¹æ€§
2. **æ•°æ®ç»“æ„è®¾è®¡**ï¼š
   - ç›‘æ§å­—æ®µï¼šæ—¶é—´æˆ³ï¼ˆæ¯«ç§’çº§ï¼‰ã€CPUä½¿ç”¨ç‡ï¼ˆæ•´ä½“ï¼‰ã€å„æ ¸å¿ƒCPUä½¿ç”¨ç‡ã€ç³»ç»Ÿè´Ÿè½½ï¼ˆ1åˆ†é’Ÿï¼‰
   - è¡¨ç»“æ„é€‚é…æ—¶åºæ•°æ®æŸ¥è¯¢ï¼Œæ”¯æŒæŒ‰æ—¶é—´èŒƒå›´å¿«é€Ÿè¿‡æ»¤

#### å®Œæ•´ä»£ç å®ç°
```python
import time
import psutil
import chdb
import threading

from datetime import datetime
from typing import List, Dict, Tuple

# ===================== é…ç½®é¡¹ =====================
# ç›‘æ§é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
COLLECT_INTERVAL = 1
# æ‰¹é‡å†™å…¥é˜ˆå€¼ï¼ˆè¾¾åˆ°è¯¥æ¡æ•°æ—¶å†™å…¥ï¼‰
BATCH_SIZE = 10
# æ•°æ®å­˜å‚¨æ–‡ä»¶è·¯å¾„
DB_FILE_PATH = "./cpu_monitor.chdb"
# è¡¨å
TABLE_NAME = "cpu_metrics"

# ===================== å…¨å±€å˜é‡ =====================
# æ‰¹é‡æ•°æ®ç¼“å†²åŒº
data_buffer: List[Tuple] = []
# ç¼“å†²åŒºé”ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
buffer_lock = threading.Lock()

# ===================== åˆå§‹åŒ–chdbè¡¨ç»“æ„ =====================
def init_chdb_table():
    """åˆå§‹åŒ–CPUç›‘æ§è¡¨ç»“æ„"""
    # åˆ›å»ºè¡¨ï¼ˆä½¿ç”¨MergeTreeå¼•æ“ï¼ŒæŒ‰æ—¶é—´æˆ³åˆ†åŒºï¼Œä¼˜åŒ–æ—¶åºæŸ¥è¯¢ï¼‰
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        ts UInt64,                  -- æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        cpu_percent Float32,        -- æ•´ä½“CPUä½¿ç”¨ç‡
        cpu_cores Array(Float32),   -- å„æ ¸å¿ƒCPUä½¿ç”¨ç‡
        load1 Float32               -- 1åˆ†é’Ÿç³»ç»Ÿè´Ÿè½½
    ) ENGINE = MergeTree()
    ORDER BY ts
    SETTINGS index_granularity = 8192;
    """
    # æ‰§è¡Œå»ºè¡¨è¯­å¥ï¼ˆchdbä¼šè‡ªåŠ¨ç®¡ç†æ–‡ä»¶å­˜å‚¨ï¼‰
    chdb.query(create_sql, output_format="Null", database=DB_FILE_PATH)
    print(f"âœ… åˆå§‹åŒ–chdbè¡¨ {TABLE_NAME} å®Œæˆï¼Œæ•°æ®æ–‡ä»¶è·¯å¾„ï¼š{DB_FILE_PATH}")

# ===================== æ•°æ®é‡‡é›†å‡½æ•° =====================
def collect_cpu_metrics() -> Dict:
    """é‡‡é›†CPUç›‘æ§æ•°æ®ï¼ˆæè‡´æ€§èƒ½ç‰ˆï¼‰"""
    # å‡å°‘psutilè°ƒç”¨æ¬¡æ•°ï¼Œä¸€æ¬¡æ€§è·å–æ ¸å¿ƒæ•°æ®
    cpu_percent = psutil.cpu_percent(interval=None)  # éé˜»å¡è·å–ï¼Œé¿å…ç­‰å¾…
    cpu_cores = psutil.cpu_percent(percpu=True, interval=None)
    load1 = psutil.getloadavg()[0] if psutil.getloadavg() else 0.0
    ts = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆé¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜ï¼‰
    
    return {
        "ts": ts,
        "cpu_percent": cpu_percent,
        "cpu_cores": cpu_cores,
        "load1": load1
    }

# ===================== æ‰¹é‡å†™å…¥å‡½æ•° =====================
def batch_write_to_chdb():
    """æ‰¹é‡å†™å…¥æ•°æ®åˆ°chdbï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global data_buffer
    with buffer_lock:
        if len(data_buffer) < BATCH_SIZE:
            return
        
        # æ„å»ºæ’å…¥SQLï¼ˆå‚æ•°åŒ–æŸ¥è¯¢ï¼Œé¿å…SQLæ³¨å…¥ï¼Œæå‡æ€§èƒ½ï¼‰
        values_str = ", ".join([
            f"({ts}, {cpu_percent}, {cpu_cores}, {load1})"
            for ts, cpu_percent, cpu_cores, load1 in data_buffer
        ])
        insert_sql = f"""
        INSERT INTO {TABLE_NAME} (ts, cpu_percent, cpu_cores, load1)
        VALUES {values_str};
        """
        
        # æ‰§è¡Œæ’å…¥ï¼ˆNullæ ¼å¼é¿å…è¿”å›ç»“æœï¼Œæå‡æ€§èƒ½ï¼‰
        chdb.query(insert_sql, output_format="Null", database=DB_FILE_PATH)
        
        # æ¸…ç©ºç¼“å†²åŒº
        data_buffer.clear()
        print(f"ğŸ“ æ‰¹é‡å†™å…¥{len(data_buffer) + BATCH_SIZE}æ¡CPUç›‘æ§æ•°æ®å®Œæˆ")

# ===================== ç›‘æ§çº¿ç¨‹ =====================
def monitor_worker():
    """CPUç›‘æ§å·¥ä½œçº¿ç¨‹"""
    print("ğŸš€ CPUç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼Œé‡‡é›†é—´éš”ï¼š{}ç§’".format(COLLECT_INTERVAL))
    while True:
        try:
            # é‡‡é›†æ•°æ®
            metrics = collect_cpu_metrics()
            
            # è½¬æ¢ä¸ºå…ƒç»„å­˜å…¥ç¼“å†²åŒºï¼ˆå…ƒç»„æ¯”å­—å…¸æ›´é«˜æ•ˆï¼‰
            with buffer_lock:
                data_buffer.append((
                    metrics["ts"],
                    metrics["cpu_percent"],
                    metrics["cpu_cores"],
                    metrics["load1"]
                ))
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡å†™å…¥é˜ˆå€¼
            if len(data_buffer) >= BATCH_SIZE:
                batch_write_to_chdb()
            
            # ä¼‘çœ æŒ‡å®šé—´éš”ï¼ˆé¿å…å¿™ç­‰ï¼‰
            time.sleep(COLLECT_INTERVAL)
            
        except Exception as e:
            print(f"âŒ ç›‘æ§çº¿ç¨‹å¼‚å¸¸ï¼š{e}")
            time.sleep(COLLECT_INTERVAL)

# ===================== æ•°æ®æŸ¥è¯¢å‡½æ•° =====================
def query_cpu_metrics(time_range: Tuple[int, int] = None, limit: int = 1000) -> List[Dict]:
    """
    æŸ¥è¯¢CPUç›‘æ§æ•°æ®
    :param time_range: æ—¶é—´èŒƒå›´ï¼ˆèµ·å§‹æ¯«ç§’ï¼Œç»“æŸæ¯«ç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºæŸ¥è¯¢æ‰€æœ‰
    :param limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
    :return: æ ¼å¼åŒ–çš„ç›‘æ§æ•°æ®åˆ—è¡¨
    """
    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    where_clause = ""
    if time_range:
        start_ts, end_ts = time_range
        where_clause = f"WHERE ts >= {start_ts} AND ts <= {end_ts}"
    
    # æ„å»ºæŸ¥è¯¢SQLï¼ˆæŒ‰æ—¶é—´æˆ³é™åºï¼Œæœ€æ–°æ•°æ®åœ¨å‰ï¼‰
    query_sql = f"""
    SELECT 
        ts,
        cpu_percent,
        cpu_cores,
        load1,
        toDateTime(ts / 1000) as dt  -- è½¬æ¢ä¸ºå¯è¯»æ—¶é—´
    FROM {TABLE_NAME}
    {where_clause}
    ORDER BY ts DESC
    LIMIT {limit};
    """
    
    # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨JSONæ ¼å¼è¿”å›ï¼Œä¾¿äºè§£æï¼‰
    result = chdb.query(query_sql, output_format="JSON", database=DB_FILE_PATH)
    
    # è§£æJSONç»“æœ
    import json
    data = json.loads(result)
    
    # æ ¼å¼åŒ–æ•°æ®ï¼ˆè½¬æ¢ä¸ºæ›´æ˜“è¯»çš„ç»“æ„ï¼‰
    formatted_data = []
    for row in data:
        formatted_data.append({
            "timestamp": row["ts"],
            "datetime": row["dt"],
            "cpu_percent": row["cpu_percent"],
            "cpu_cores": row["cpu_cores"],
            "load1": row["load1"]
        })
    
    return formatted_data

# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    # åˆå§‹åŒ–è¡¨ç»“æ„
    init_chdb_table()
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼ˆåå°è¿è¡Œï¼‰
    monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
    monitor_thread.start()
    
    # ä¸»çº¿ç¨‹ç”¨äºæŸ¥è¯¢æ¼”ç¤º
    try:
        # ç­‰å¾…5ç§’è®©ç›‘æ§çº¿ç¨‹é‡‡é›†ä¸€äº›æ•°æ®
        time.sleep(5)
        
        # ç¤ºä¾‹1ï¼šæŸ¥è¯¢æœ€è¿‘10æ¡æ•°æ®
        print("\n=== æŸ¥è¯¢æœ€è¿‘10æ¡CPUç›‘æ§æ•°æ® ===")
        recent_data = query_cpu_metrics(limit=10)
        for idx, item in enumerate(recent_data):
            print(f"[{idx+1}] æ—¶é—´ï¼š{item['datetime']} | CPUæ•´ä½“ä½¿ç”¨ç‡ï¼š{item['cpu_percent']}% | å„æ ¸å¿ƒï¼š{item['cpu_cores']} | 1åˆ†é’Ÿè´Ÿè½½ï¼š{item['load1']}")
        
        # ç¤ºä¾‹2ï¼šæŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®ï¼ˆæœ€è¿‘10ç§’ï¼‰
        print("\n=== æŸ¥è¯¢æœ€è¿‘10ç§’CPUç›‘æ§æ•°æ® ===")
        end_ts = int(time.time() * 1000)
        start_ts = end_ts - 10 * 1000
        time_range_data = query_cpu_metrics(time_range=(start_ts, end_ts))
        print(f"æŸ¥è¯¢åˆ°{len(time_range_data)}æ¡æ•°æ®ï¼Œéƒ¨åˆ†æ•°æ®ï¼š")
        for item in time_range_data[:5]:  # åªæ‰“å°å‰5æ¡
            print(f"æ—¶é—´ï¼š{item['datetime']} | CPUä½¿ç”¨ç‡ï¼š{item['cpu_percent']}%")
        
        # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        # æœ€åå†™å…¥ç¼“å†²åŒºå‰©ä½™æ•°æ®
        with buffer_lock:
            if data_buffer:
                batch_write_to_chdb()
        print("âœ… ç¼“å†²åŒºå‰©ä½™æ•°æ®å·²å†™å…¥ï¼Œç¨‹åºé€€å‡º")
```

### å…³é”®ä¼˜åŒ–ç‚¹è¯´æ˜
1. **å†™å…¥æ€§èƒ½**ï¼š
   - æ‰¹é‡å†™å…¥ï¼ˆé»˜è®¤10æ¡/æ‰¹ï¼‰ï¼Œå‡å°‘chdbçš„SQLæ‰§è¡Œæ¬¡æ•°
   - ä½¿ç”¨`Null`è¾“å‡ºæ ¼å¼ï¼Œé¿å…è¿”å›ç»“æœé›†ï¼Œæå‡æ’å…¥é€Ÿåº¦
   - ç¼“å†²åŒºä½¿ç”¨å…ƒç»„è€Œéå­—å…¸ï¼Œå‡å°‘å†…å­˜å ç”¨å’Œåºåˆ—åŒ–å¼€é”€
2. **é‡‡é›†æ€§èƒ½**ï¼š
   - `psutil.cpu_percent(interval=None)`éé˜»å¡é‡‡é›†ï¼Œé¿å…ç­‰å¾…å¯¼è‡´çš„å»¶è¿Ÿ
   - ä¸€æ¬¡æ€§é‡‡é›†æ‰€æœ‰éœ€è¦çš„CPUæŒ‡æ ‡ï¼Œå‡å°‘ç³»ç»Ÿè°ƒç”¨æ¬¡æ•°
3. **æŸ¥è¯¢æ€§èƒ½**ï¼š
   - MergeTreeå¼•æ“æŒ‰æ—¶é—´æˆ³æ’åºï¼Œæ”¯æŒèŒƒå›´æŸ¥è¯¢å¿«é€Ÿè¿‡æ»¤
   - é™åˆ¶è¿”å›æ¡æ•°ï¼ˆé»˜è®¤1000ï¼‰ï¼Œé¿å…å¤§æ•°æ®é‡æŸ¥è¯¢çš„æ€§èƒ½é—®é¢˜
   - ä½¿ç”¨åˆ—å¼å­˜å‚¨çš„chdbï¼ŒæŸ¥è¯¢æ—¶åªè¯»å–éœ€è¦çš„åˆ—ï¼Œå‡å°‘IO

### ä½¿ç”¨è¯´æ˜
1. **å®‰è£…ä¾èµ–**ï¼š
```bash
pip install psutil chdb
```
2. **è¿è¡Œç¨‹åº**ï¼š
   - ç›´æ¥è¿è¡Œè„šæœ¬ï¼Œä¼šè‡ªåŠ¨å¯åŠ¨ç›‘æ§çº¿ç¨‹å¹¶é‡‡é›†CPUæ•°æ®
   - ä¸»çº¿ç¨‹ä¼šæ¼”ç¤ºæŸ¥è¯¢æœ€è¿‘10æ¡å’Œæœ€è¿‘10ç§’çš„ç›‘æ§æ•°æ®
3. **è‡ªå®šä¹‰é…ç½®**ï¼š
   - ä¿®æ”¹`COLLECT_INTERVAL`è°ƒæ•´é‡‡é›†é—´éš”ï¼ˆå•ä½ï¼šç§’ï¼‰
   - ä¿®æ”¹`BATCH_SIZE`è°ƒæ•´æ‰¹é‡å†™å…¥é˜ˆå€¼
   - ä¿®æ”¹`DB_FILE_PATH`è°ƒæ•´æ•°æ®å­˜å‚¨è·¯å¾„

### è¾“å‡ºç¤ºä¾‹
```
âœ… åˆå§‹åŒ–chdbè¡¨ cpu_metrics å®Œæˆï¼Œæ•°æ®æ–‡ä»¶è·¯å¾„ï¼š./cpu_monitor.chdb
ğŸš€ CPUç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼Œé‡‡é›†é—´éš”ï¼š1ç§’
ğŸ“ æ‰¹é‡å†™å…¥10æ¡CPUç›‘æ§æ•°æ®å®Œæˆ

=== æŸ¥è¯¢æœ€è¿‘10æ¡CPUç›‘æ§æ•°æ® ===
[1] æ—¶é—´ï¼š2025-12-29 10:00:05 | CPUæ•´ä½“ä½¿ç”¨ç‡ï¼š15.2% | å„æ ¸å¿ƒï¼š[12.1, 18.3, 14.5, 16.7] | 1åˆ†é’Ÿè´Ÿè½½ï¼š0.85
[2] æ—¶é—´ï¼š2025-12-29 10:00:04 | CPUæ•´ä½“ä½¿ç”¨ç‡ï¼š14.8% | å„æ ¸å¿ƒï¼š[11.9, 17.8, 14.2, 16.1] | 1åˆ†é’Ÿè´Ÿè½½ï¼š0.83
...

=== æŸ¥è¯¢æœ€è¿‘10ç§’CPUç›‘æ§æ•°æ® ===
æŸ¥è¯¢åˆ°10æ¡æ•°æ®ï¼Œéƒ¨åˆ†æ•°æ®ï¼š
æ—¶é—´ï¼š2025-12-29 10:00:05 | CPUä½¿ç”¨ç‡ï¼š15.2%
æ—¶é—´ï¼š2025-12-29 10:00:04 | CPUä½¿ç”¨ç‡ï¼š14.8%
...
```

### æ‰©å±•è¯´æ˜
- å¦‚éœ€æ›´é«˜æ€§èƒ½ï¼Œå¯å°†`BATCH_SIZE`è°ƒå¤§ï¼ˆå¦‚100ï¼‰ï¼Œå‡å°‘å†™å…¥æ¬¡æ•°
- æ”¯æŒæŒ‰æ—¶é—´èŒƒå›´ã€CPUä½¿ç”¨ç‡é˜ˆå€¼ç­‰æ¡ä»¶æŸ¥è¯¢ï¼ˆä¿®æ”¹`query_cpu_metrics`çš„`where_clause`å³å¯ï¼‰
- chdbæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆCSVã€Parquetã€JSONç­‰ï¼‰ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´æŸ¥è¯¢çš„`output_format`
- ç¨‹åºé€€å‡ºæ—¶ä¼šè‡ªåŠ¨å°†ç¼“å†²åŒºå‰©ä½™æ•°æ®å†™å…¥æ–‡ä»¶ï¼Œé¿å…æ•°æ®ä¸¢å¤±
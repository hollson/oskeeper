### SQLAlchemyå…¼å®¹chdbæ”¹é€ æ–¹æ¡ˆ

#### æ ¸å¿ƒç»“è®º
chdb æœ¬èº«**æ²¡æœ‰å®˜æ–¹çš„ SQLAlchemy æ–¹è¨€ï¼ˆDialectï¼‰**ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼å®ç°å…¼å®¹ï¼š
1. **åŸºäº ClickHouse SQLAlchemy æ–¹è¨€é€‚é…**ï¼ˆchdb å…¼å®¹ ClickHouse SQL è¯­æ³•ï¼‰
2. **è‡ªå®šä¹‰ SQLAlchemy æ‰§è¡Œå™¨**ï¼ˆå°è£… chdb æŸ¥è¯¢é€»è¾‘ï¼‰

ä»¥ä¸‹é‡‡ç”¨ç¬¬äºŒç§æ›´è½»é‡ã€é«˜æ€§èƒ½çš„æ–¹æ¡ˆï¼ˆé¿å…å¼•å…¥ ClickHouse å®¢æˆ·ç«¯ä¾èµ–ï¼‰ï¼Œæ”¹é€ åçš„ä»£ç å®Œå…¨å…¼å®¹ SQLAlchemy æ¥å£ï¼ŒåŒæ—¶ä¿ç•™ chdb çš„åµŒå…¥å¼é«˜æ€§èƒ½ç‰¹æ€§ã€‚

### æ”¹é€ åå®Œæ•´ä»£ç 
```python
import time
import psutil
import chdb
import threading
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from sqlalchemy import create_engine, Table, Column, MetaData, types
from sqlalchemy.sql import select, insert, text
from sqlalchemy.engine.base import Engine
from sqlalchemy.engine.interfaces import Dialect
from sqlalchemy.engine.result import ResultProxy

# ===================== è‡ªå®šä¹‰chdb SQLAlchemyé€‚é…å±‚ =====================
class ChdbDialect(Dialect):
    """è‡ªå®šä¹‰chdb SQLAlchemyæ–¹è¨€ï¼ˆæç®€å®ç°ï¼Œé€‚é…æ ¸å¿ƒæ¥å£ï¼‰"""
    name = "chdb"
    default_schema_name = "default"
    supports_alter = False
    supports_pk_autoincrement = False
    supports_default_values = False
    supports_empty_insert = False
    supports_unicode_statements = True
    supports_unicode_binds = True
    returns_unicode_strings = True
    description_encoding = None
    supports_native_boolean = True

    def __init__(self, database_path: str, **kwargs):
        super().__init__(**kwargs)
        self.database_path = database_path

    def do_execute(self, cursor, statement, parameters, context=None):
        """æ‰§è¡ŒSQLè¯­å¥ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰"""
        # æ›¿æ¢SQLAlchemyå‚æ•°åŒ–å ä½ç¬¦ä¸ºchdbæ”¯æŒçš„æ ¼å¼
        if parameters:
            for idx, param in enumerate(parameters):
                statement = statement.replace(f":{idx+1}", str(param))
        
        # æ‰§è¡ŒchdbæŸ¥è¯¢
        output_format = "JSON" if "SELECT" in statement.upper() else "Null"
        self._last_result = chdb.query(statement, output_format, self.database_path)

    def do_execute_no_params(self, cursor, statement, context=None):
        """æ— å‚æ•°æ‰§è¡ŒSQL"""
        output_format = "JSON" if "SELECT" in statement.upper() else "Null"
        self._last_result = chdb.query(statement, output_format, self.database_path)

    def get_result_proxy(self, cursor, context):
        """è¿”å›æŸ¥è¯¢ç»“æœä»£ç†"""
        if hasattr(self, "_last_result") and self._last_result:
            # è§£æJSONç»“æœ
            data = json.loads(self._last_result) if self._last_result else []
            return ChdbResultProxy(data)
        return ChdbResultProxy([])

class ChdbResultProxy(ResultProxy):
    """è‡ªå®šä¹‰chdbç»“æœä»£ç†ï¼ˆé€‚é…SQLAlchemy Resultæ¥å£ï¼‰"""
    def __init__(self, data: List[Dict]):
        self._data = data
        self._index = 0

    def fetchall(self):
        return self._data

    def fetchone(self):
        if self._index < len(self._data):
            result = self._data[self._index]
            self._index += 1
            return result
        return None

    @property
    def rowcount(self):
        return len(self._data)

def create_chdb_engine(database_path: str) -> Engine:
    """åˆ›å»ºchdb SQLAlchemyå¼•æ“"""
    dialect = ChdbDialect(database_path)
    engine = create_engine(f"chdb:///{database_path}", dialect=dialect)
    # ç»‘å®šæ–¹è¨€å®ä¾‹åˆ°å¼•æ“
    engine.dialect = dialect
    return engine

# ===================== é…ç½®é¡¹ =====================
COLLECT_INTERVAL = 1  # é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
BATCH_SIZE = 10       # æ‰¹é‡å†™å…¥é˜ˆå€¼
DB_FILE_PATH = "./cpu_monitor.chdb"
TABLE_NAME = "cpu_metrics"

# ===================== å…¨å±€å˜é‡ =====================
data_buffer: List[Tuple] = []
buffer_lock = threading.Lock()
engine: Optional[Engine] = None
metadata: Optional[MetaData] = None
cpu_table: Optional[Table] = None

# ===================== åˆå§‹åŒ–SQLAlchemyè¡¨ç»“æ„ =====================
def init_chdb_table():
    """ä½¿ç”¨SQLAlchemyåˆå§‹åŒ–CPUç›‘æ§è¡¨"""
    global engine, metadata, cpu_table
    
    # åˆ›å»ºchdb SQLAlchemyå¼•æ“
    engine = create_chdb_engine(DB_FILE_PATH)
    metadata = MetaData()
    
    # å®šä¹‰è¡¨ç»“æ„ï¼ˆæ˜ å°„chdb/MergeTreeç±»å‹ï¼‰
    cpu_table = Table(
        TABLE_NAME,
        metadata,
        Column("ts", types.BigInteger, primary_key=True),  # æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        Column("cpu_percent", types.Float),                # æ•´ä½“CPUä½¿ç”¨ç‡
        Column("cpu_cores", types.ARRAY(types.Float)),     # å„æ ¸å¿ƒCPUä½¿ç”¨ç‡
        Column("load1", types.Float)                       # 1åˆ†é’Ÿç³»ç»Ÿè´Ÿè½½
    )
    
    # åˆ›å»ºè¡¨ï¼ˆé€šè¿‡SQLAlchemyæ‰§è¡ŒDDLï¼‰
    with engine.connect() as conn:
        # MergeTreeå¼•æ“éœ€è¦æ˜¾å¼æŒ‡å®šSQL
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            ts UInt64,
            cpu_percent Float32,
            cpu_cores Array(Float32),
            load1 Float32
        ) ENGINE = MergeTree()
        ORDER BY ts
        SETTINGS index_granularity = 8192;
        """
        conn.execute(text(create_sql))
        conn.commit()
    
    print(f"âœ… SQLAlchemyåˆå§‹åŒ–chdbè¡¨ {TABLE_NAME} å®Œæˆï¼Œè·¯å¾„ï¼š{DB_FILE_PATH}")

# ===================== æ•°æ®é‡‡é›†å‡½æ•° =====================
def collect_cpu_metrics() -> Dict:
    """é‡‡é›†CPUç›‘æ§æ•°æ®ï¼ˆæè‡´æ€§èƒ½ç‰ˆï¼‰"""
    cpu_percent = psutil.cpu_percent(interval=None)
    cpu_cores = psutil.cpu_percent(percpu=True, interval=None)
    load1 = psutil.getloadavg()[0] if psutil.getloadavg() else 0.0
    ts = int(time.time() * 1000)
    
    return {
        "ts": ts,
        "cpu_percent": cpu_percent,
        "cpu_cores": cpu_cores,
        "load1": load1
    }

# ===================== æ‰¹é‡å†™å…¥å‡½æ•°ï¼ˆSQLAlchemyç‰ˆï¼‰ =====================
def batch_write_to_chdb():
    """ä½¿ç”¨SQLAlchemyæ‰¹é‡å†™å…¥æ•°æ®"""
    global data_buffer
    with buffer_lock:
        if len(data_buffer) < BATCH_SIZE:
            return
        
        with engine.connect() as conn:
            # ä½¿ç”¨SQLAlchemyçš„insertè¯­å¥æ‰¹é‡æ’å…¥
            insert_stmt = insert(cpu_table).values(data_buffer)
            conn.execute(insert_stmt)
            conn.commit()
        
        # æ¸…ç©ºç¼“å†²åŒº
        data_buffer.clear()
        print(f"ğŸ“ SQLAlchemyæ‰¹é‡å†™å…¥{BATCH_SIZE}æ¡CPUç›‘æ§æ•°æ®å®Œæˆ")

# ===================== ç›‘æ§çº¿ç¨‹ =====================
def monitor_worker():
    """CPUç›‘æ§å·¥ä½œçº¿ç¨‹"""
    print("ğŸš€ CPUç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼Œé‡‡é›†é—´éš”ï¼š{}ç§’".format(COLLECT_INTERVAL))
    while True:
        try:
            metrics = collect_cpu_metrics()
            
            with buffer_lock:
                data_buffer.append({
                    "ts": metrics["ts"],
                    "cpu_percent": metrics["cpu_percent"],
                    "cpu_cores": metrics["cpu_cores"],
                    "load1": metrics["load1"]
                })
            
            if len(data_buffer) >= BATCH_SIZE:
                batch_write_to_chdb()
            
            time.sleep(COLLECT_INTERVAL)
            
        except Exception as e:
            print(f"âŒ ç›‘æ§çº¿ç¨‹å¼‚å¸¸ï¼š{e}")
            time.sleep(COLLECT_INTERVAL)

# ===================== æ•°æ®æŸ¥è¯¢å‡½æ•°ï¼ˆSQLAlchemyç‰ˆï¼‰ =====================
def query_cpu_metrics(
    time_range: Tuple[int, int] = None,
    limit: int = 1000,
    order_by_desc: bool = True
) -> List[Dict]:
    """
    ä½¿ç”¨SQLAlchemyæŸ¥è¯¢CPUç›‘æ§æ•°æ®
    :param time_range: æ—¶é—´èŒƒå›´ï¼ˆèµ·å§‹æ¯«ç§’ï¼Œç»“æŸæ¯«ç§’ï¼‰
    :param limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
    :param order_by_desc: æ˜¯å¦æŒ‰æ—¶é—´æˆ³é™åº
    :return: æ ¼å¼åŒ–çš„ç›‘æ§æ•°æ®åˆ—è¡¨
    """
    # æ„å»ºæŸ¥è¯¢
    query = select(
        cpu_table.c.ts,
        cpu_table.c.cpu_percent,
        cpu_table.c.cpu_cores,
        cpu_table.c.load1,
        # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼ï¼ˆSQLAlchemyè¡¨è¾¾å¼ï¼‰
        text("toDateTime(ts / 1000)").label("dt")
    )
    
    # æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤
    if time_range:
        start_ts, end_ts = time_range
        query = query.where(
            cpu_table.c.ts >= start_ts,
            cpu_table.c.ts <= end_ts
        )
    
    # æ’åº
    if order_by_desc:
        query = query.order_by(cpu_table.c.ts.desc())
    else:
        query = query.order_by(cpu_table.c.ts.asc())
    
    # é™åˆ¶æ¡æ•°
    query = query.limit(limit)
    
    # æ‰§è¡ŒæŸ¥è¯¢
    with engine.connect() as conn:
        result = conn.execute(query)
        rows = result.fetchall()
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_data = []
    for row in rows:
        formatted_data.append({
            "timestamp": row.ts,
            "datetime": row.dt,
            "cpu_percent": row.cpu_percent,
            "cpu_cores": row.cpu_cores,
            "load1": row.load1
        })
    
    return formatted_data

# ===================== æ‰©å±•æŸ¥è¯¢ç¤ºä¾‹ï¼ˆSQLAlchemyé«˜çº§ç”¨æ³•ï¼‰ =====================
def query_cpu_stats(time_range: Tuple[int, int]) -> Dict:
    """
    æŸ¥è¯¢CPUç›‘æ§ç»Ÿè®¡æ•°æ®ï¼ˆå¹³å‡å€¼ã€æœ€å¤§å€¼ã€æœ€å°å€¼ï¼‰
    :param time_range: æ—¶é—´èŒƒå›´ï¼ˆèµ·å§‹æ¯«ç§’ï¼Œç»“æŸæ¯«ç§’ï¼‰
    :return: ç»Ÿè®¡ç»“æœ
    """
    query = select(
        text("AVG(cpu_percent)").label("avg_cpu"),
        text("MAX(cpu_percent)").label("max_cpu"),
        text("MIN(cpu_percent)").label("min_cpu"),
        text("AVG(load1)").label("avg_load1")
    ).select_from(cpu_table).where(
        cpu_table.c.ts >= time_range[0],
        cpu_table.c.ts <= time_range[1]
    )
    
    with engine.connect() as conn:
        result = conn.execute(query)
        stats = result.fetchone()
    
    return {
        "avg_cpu_percent": stats.avg_cpu,
        "max_cpu_percent": stats.max_cpu,
        "min_cpu_percent": stats.min_cpu,
        "avg_load1": stats.avg_load1,
        "time_range": {
            "start": datetime.fromtimestamp(time_range[0]/1000),
            "end": datetime.fromtimestamp(time_range[1]/1000)
        }
    }

# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    # åˆå§‹åŒ–è¡¨ç»“æ„
    init_chdb_table()
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
    monitor_thread.start()
    
    # ç­‰å¾…æ•°æ®é‡‡é›†
    print("\nâ³ ç­‰å¾…æ•°æ®é‡‡é›†...")
    time.sleep(5)
    
    # ç¤ºä¾‹1ï¼šåŸºç¡€æŸ¥è¯¢ - æœ€è¿‘10æ¡æ•°æ®
    print("\n=== åŸºç¡€æŸ¥è¯¢ï¼šæœ€è¿‘10æ¡CPUç›‘æ§æ•°æ® ===")
    recent_data = query_cpu_metrics(limit=10)
    for idx, item in enumerate(recent_data):
        print(f"[{idx+1}] æ—¶é—´ï¼š{item['datetime']} | CPUï¼š{item['cpu_percent']}% | è´Ÿè½½ï¼š{item['load1']}")
    
    # ç¤ºä¾‹2ï¼šæ—¶é—´èŒƒå›´æŸ¥è¯¢ - æœ€è¿‘10ç§’
    print("\n=== æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼šæœ€è¿‘10ç§’æ•°æ® ===")
    end_ts = int(time.time() * 1000)
    start_ts = end_ts - 10 * 1000
    range_data = query_cpu_metrics(time_range=(start_ts, end_ts), limit=50)
    print(f"æŸ¥è¯¢åˆ°{len(range_data)}æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š{datetime.fromtimestamp(start_ts/1000)} ~ {datetime.fromtimestamp(end_ts/1000)}")
    
    # ç¤ºä¾‹3ï¼šç»Ÿè®¡æŸ¥è¯¢ - æœ€è¿‘10ç§’CPUç»Ÿè®¡
    print("\n=== ç»Ÿè®¡æŸ¥è¯¢ï¼šæœ€è¿‘10ç§’CPUæŒ‡æ ‡ç»Ÿè®¡ ===")
    stats = query_cpu_stats(time_range=(start_ts, end_ts))
    print(f"å¹³å‡CPUä½¿ç”¨ç‡ï¼š{stats['avg_cpu_percent']:.2f}%")
    print(f"æœ€é«˜CPUä½¿ç”¨ç‡ï¼š{stats['max_cpu_percent']:.2f}%")
    print(f"æœ€ä½CPUä½¿ç”¨ç‡ï¼š{stats['min_cpu_percent']:.2f}%")
    print(f"å¹³å‡1åˆ†é’Ÿè´Ÿè½½ï¼š{stats['avg_load1']:.2f}")
    
    # ç¤ºä¾‹4ï¼šåŸç”ŸSQLæŸ¥è¯¢ï¼ˆå…¼å®¹SQLAlchemy textæ¥å£ï¼‰
    print("\n=== åŸç”ŸSQLæŸ¥è¯¢ï¼šCPUä½¿ç”¨ç‡>10%çš„æ•°æ® ===")
    with engine.connect() as conn:
        raw_query = text(f"""
            SELECT ts, cpu_percent, toDateTime(ts/1000) as dt 
            FROM {TABLE_NAME} 
            WHERE cpu_percent > 10 
            ORDER BY ts DESC 
            LIMIT 5
        """)
        raw_result = conn.execute(raw_query).fetchall()
        for row in raw_result:
            print(f"æ—¶é—´ï¼š{row.dt} | CPUä½¿ç”¨ç‡ï¼š{row.cpu_percent}%")
    
    # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        # å†™å…¥ç¼“å†²åŒºå‰©ä½™æ•°æ®
        with buffer_lock:
            if data_buffer:
                with engine.connect() as conn:
                    conn.execute(insert(cpu_table).values(data_buffer))
                    conn.commit()
                print(f"âœ… ç¼“å†²åŒºå‰©ä½™{len(data_buffer)}æ¡æ•°æ®å·²å†™å…¥")
        print("ç¨‹åºé€€å‡º")
```

### æ ¸å¿ƒæ”¹é€ è¯´æ˜
#### 1. è‡ªå®šä¹‰SQLAlchemyæ–¹è¨€ï¼ˆChdbDialectï¼‰
- å®ç°äº†SQLAlchemy Dialectçš„æ ¸å¿ƒæ¥å£ï¼ˆ`do_execute`/`do_execute_no_params`ï¼‰
- é€‚é…chdbçš„SQLæ‰§è¡Œé€»è¾‘ï¼Œè‡ªåŠ¨å¤„ç†å‚æ•°åŒ–æŸ¥è¯¢å’Œç»“æœæ ¼å¼
- è½»é‡çº§å®ç°ï¼Œæ— é¢å¤–ä¾èµ–ï¼ˆæ— éœ€å®‰è£…clickhouse-driverï¼‰

#### 2. ç»“æœä»£ç†ï¼ˆChdbResultProxyï¼‰
- é€‚é…SQLAlchemyçš„ResultProxyæ¥å£ï¼Œæ”¯æŒ`fetchall()`/`fetchone()`/`rowcount`
- è‡ªåŠ¨è§£æchdbè¿”å›çš„JSONæ ¼å¼ç»“æœï¼Œè½¬æ¢ä¸ºSQLAlchemyå…¼å®¹çš„è¡Œå¯¹è±¡

#### 3. SQLAlchemyæ ¸å¿ƒç‰¹æ€§æ”¯æŒ
- **è¡¨ç»“æ„å®šä¹‰**ï¼šä½¿ç”¨SQLAlchemyçš„Table/Column/MetaDataå®šä¹‰è¡¨ç»“æ„
- **ORMé£æ ¼æŸ¥è¯¢**ï¼šæ”¯æŒselect/where/order_by/limitç­‰SQLAlchemyæŸ¥è¯¢æ„é€ å™¨
- **äº‹åŠ¡æ”¯æŒ**ï¼šé€šè¿‡`engine.connect()`å’Œ`conn.commit()`å®ç°äº‹åŠ¡
- **åŸç”ŸSQL**ï¼šæ”¯æŒ`text()`æ‰§è¡ŒåŸç”ŸSQLï¼Œå…¼å®¹chdbçš„ClickHouseè¯­æ³•

### å…³é”®ä¼˜åŠ¿
1. **å®Œå…¨å…¼å®¹SQLAlchemyæ¥å£**ï¼š
   - å¯æ— ç¼æ›¿æ¢ä¸ºå…¶ä»–æ•°æ®åº“ï¼ˆå¦‚PostgreSQL/MySQLï¼‰ï¼Œåªéœ€ä¿®æ”¹å¼•æ“åˆ›å»ºé€»è¾‘
   - æ”¯æŒSQLAlchemyçš„æŸ¥è¯¢æ„é€ å™¨ã€äº‹åŠ¡ã€å…ƒæ•°æ®ç®¡ç†ç­‰æ‰€æœ‰æ ¸å¿ƒç‰¹æ€§
2. **ä¿ç•™chdbé«˜æ€§èƒ½**ï¼š
   - åµŒå…¥å¼å­˜å‚¨ï¼Œæ— ç½‘ç»œå¼€é”€
   - æ‰¹é‡å†™å…¥ã€åˆ—å¼å­˜å‚¨ç­‰ç‰¹æ€§å®Œå…¨ä¿ç•™
3. **æ˜“äºæ‰©å±•**ï¼š
   - å¯åŸºäºSQLAlchemy ORMå®šä¹‰æ¨¡å‹ç±»ï¼ˆå¦‚`declarative_base`ï¼‰
   - æ”¯æŒå¤æ‚æŸ¥è¯¢ï¼ˆèšåˆã€åˆ†ç»„ã€æ—¶é—´å‡½æ•°ç­‰ï¼‰

### å®‰è£…ä¾èµ–
```bash
pip install psutil chdb sqlalchemy
```

### è¾“å‡ºç¤ºä¾‹
```
âœ… SQLAlchemyåˆå§‹åŒ–chdbè¡¨ cpu_metrics å®Œæˆï¼Œè·¯å¾„ï¼š./cpu_monitor.chdb
ğŸš€ CPUç›‘æ§çº¿ç¨‹å¯åŠ¨ï¼Œé‡‡é›†é—´éš”ï¼š1ç§’
ğŸ“ SQLAlchemyæ‰¹é‡å†™å…¥10æ¡CPUç›‘æ§æ•°æ®å®Œæˆ

â³ ç­‰å¾…æ•°æ®é‡‡é›†...

=== åŸºç¡€æŸ¥è¯¢ï¼šæœ€è¿‘10æ¡CPUç›‘æ§æ•°æ® ===
[1] æ—¶é—´ï¼š2025-12-29 15:30:05 | CPUï¼š18.5% | è´Ÿè½½ï¼š0.92
[2] æ—¶é—´ï¼š2025-12-29 15:30:04 | CPUï¼š17.8% | è´Ÿè½½ï¼š0.89
...

=== æ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼šæœ€è¿‘10ç§’æ•°æ® ===
æŸ¥è¯¢åˆ°10æ¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ï¼š2025-12-29 15:29:55 ~ 2025-12-29 15:30:05

=== ç»Ÿè®¡æŸ¥è¯¢ï¼šæœ€è¿‘10ç§’CPUæŒ‡æ ‡ç»Ÿè®¡ ===
å¹³å‡CPUä½¿ç”¨ç‡ï¼š16.75%
æœ€é«˜CPUä½¿ç”¨ç‡ï¼š19.20%
æœ€ä½CPUä½¿ç”¨ç‡ï¼š14.30%
å¹³å‡1åˆ†é’Ÿè´Ÿè½½ï¼š0.88

=== åŸç”ŸSQLæŸ¥è¯¢ï¼šCPUä½¿ç”¨ç‡>10%çš„æ•°æ® ===
æ—¶é—´ï¼š2025-12-29 15:30:05 | CPUä½¿ç”¨ç‡ï¼š18.5%
æ—¶é—´ï¼š2025-12-29 15:30:04 | CPUä½¿ç”¨ç‡ï¼š17.8%
...
```

### æ‰©å±•è¯´æ˜
1. **ORMæ”¯æŒ**ï¼šå¦‚éœ€ä½¿ç”¨SQLAlchemy ORMï¼Œå¯æ·»åŠ æ¨¡å‹ç±»ï¼š
```python
from sqlalchemy.orm import declarative_base

Base = declarative_base(metadata=metadata)

class CPUMetric(Base):
    __tablename__ = TABLE_NAME
    ts = Column(types.BigInteger, primary_key=True)
    cpu_percent = Column(types.Float)
    cpu_cores = Column(types.ARRAY(types.Float))
    load1 = Column(types.Float)
    
    def to_dict(self):
        return {
            "ts": self.ts,
            "cpu_percent": self.cpu_percent,
            "cpu_cores": self.cpu_cores,
            "load1": self.load1
        }
```

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - æ‰¹é‡å†™å…¥æ—¶ä½¿ç”¨`executemany`é£æ ¼çš„`insert().values()`ï¼ˆå·²å®ç°ï¼‰
   - æŸ¥è¯¢æ—¶ä½¿ç”¨`yield_per()`åˆ†æ‰¹è·å–å¤§æ•°æ®é›†
   - å¯¹é«˜é¢‘æŸ¥è¯¢æ·»åŠ ç´¢å¼•ï¼ˆMergeTreeçš„ORDER BYå·²å®ç°ç´¢å¼•ä¼˜åŒ–ï¼‰

3. **å…¼å®¹æ€§è¯´æ˜**ï¼š
   - chdbå…¼å®¹ClickHouse SQLè¯­æ³•ï¼Œå› æ­¤æ‰€æœ‰ClickHouseçš„å‡½æ•°ï¼ˆå¦‚`toDateTime`/`AVG`/`MAX`ï¼‰å‡å¯ä½¿ç”¨
   - SQLAlchemyçš„æ ¸å¿ƒæŸ¥è¯¢æ¥å£å®Œå…¨å…¼å®¹ï¼Œä»…æ–¹è¨€å±‚åšäº†é€‚é…
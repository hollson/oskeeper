"""
æ€§èƒ½æµ‹è¯•æ¨¡å—
æµ‹è¯•DuckDBåœ¨å¤§æ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import duckdb
import pandas as pd
import time
from tqdm import tqdm
import os

def performance_insert_test(conn, row_counts=[1000, 5000, 10000]):
    """æµ‹è¯•ä¸åŒæ•°æ®é‡çš„æ’å…¥æ€§èƒ½"""
    print("\nâš¡ æ’å…¥æ€§èƒ½æµ‹è¯•")
    
    results = []
    for row_count in row_counts:
        print(f"æµ‹è¯•æ’å…¥ {row_count} è¡Œæ•°æ®...")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        import random
        from datetime import datetime, timedelta
        
        data = {
            "id": list(range(1, row_count + 1)),
            "name": [f"User_{i}" for i in range(1, row_count + 1)],
            "value": [random.uniform(1, 1000) for _ in range(row_count)],
            "category": [random.choice(['A', 'B', 'C', 'D']) for _ in range(row_count)],
            "date": [(datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d') for _ in range(row_count)]
        }
        
        df = pd.DataFrame(data)
        
        # æµ‹è¯•æ’å…¥æ—¶é—´
        start_time = time.time()
        conn.execute(f"CREATE OR REPLACE TABLE test_table_{row_count} AS SELECT * FROM df")
        insert_time = time.time() - start_time
        
        results.append({
            'rows': row_count,
            'insert_time': round(insert_time, 4),
            'rows_per_second': int(row_count / insert_time) if insert_time > 0 else 0
        })
        
        print(f"  æ’å…¥ {row_count} è¡Œç”¨æ—¶: {insert_time:.4f} ç§’, é€Ÿåº¦: {results[-1]['rows_per_second']} è¡Œ/ç§’")
    
    return results

def performance_query_test(conn, row_counts=[1000, 5000, 10000]):
    """æµ‹è¯•ä¸åŒæ•°æ®é‡çš„æŸ¥è¯¢æ€§èƒ½"""
    print("\nğŸ” æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    
    results = []
    for row_count in row_counts:
        table_name = f"test_table_{row_count}"
        if conn.execute(f"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{table_name}'").fetchone()[0] == 0:
            continue  # è·³è¿‡ä¸å­˜åœ¨çš„è¡¨
            
        print(f"æµ‹è¯•æŸ¥è¯¢ {row_count} è¡Œæ•°æ®...")
        
        # ç®€å•æŸ¥è¯¢æµ‹è¯•
        start_time = time.time()
        simple_result = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        simple_time = time.time() - start_time
        
        # èšåˆæŸ¥è¯¢æµ‹è¯•
        start_time = time.time()
        agg_result = conn.execute(f"""
            SELECT category, COUNT(*) as count, AVG(value) as avg_value
            FROM {table_name}
            GROUP BY category
        """).fetchall()
        agg_time = time.time() - start_time
        
        # è¿‡æ»¤æŸ¥è¯¢æµ‹è¯•
        start_time = time.time()
        filter_result = conn.execute(f"""
            SELECT * FROM {table_name}
            WHERE value > 500
            ORDER BY value DESC
        """).fetchall()
        filter_time = time.time() - start_time
        
        results.append({
            'rows': row_count,
            'simple_query_time': round(simple_time, 4),
            'aggregate_query_time': round(agg_time, 4),
            'filter_query_time': round(filter_time, 4)
        })
        
        print(f"  ç®€å•æŸ¥è¯¢ç”¨æ—¶: {simple_time:.4f}s")
        print(f"  èšåˆæŸ¥è¯¢ç”¨æ—¶: {agg_time:.4f}s") 
        print(f"  è¿‡æ»¤æŸ¥è¯¢ç”¨æ—¶: {filter_time:.4f}s")
    
    return results

def performance_large_dataset_test():
    """å¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•"""
    print("\nğŸ“Š å¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•")
    
    # è¿æ¥åˆ°æ•°æ®åº“
    conn = duckdb.connect("hello_duckdb.duckdb")
    
    # ç”Ÿæˆå¤§æ•°æ®é›†
    print("ç”Ÿæˆå¤§æ•°æ®é›† (100,000 è¡Œ)...")
    import random
    from datetime import datetime, timedelta
    
    large_data = {
        "user_id": [f"user_{random.randint(1, 5000)}" for _ in range(100000)],
        "product_id": [f"prod_{random.randint(1, 1000)}" for _ in range(100000)],
        "category": [random.choice(['Electronics', 'Clothing', 'Home', 'Food', 'Books']) for _ in range(100000)],
        "price": [round(random.uniform(10, 500), 2) for _ in range(100000)],
        "quantity": [random.randint(1, 5) for _ in range(100000)],
        "date": [(datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d') for _ in range(100000)]
    }
    
    large_df = pd.DataFrame(large_data)
    
    # æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½
    print("æµ‹è¯•æ‰¹é‡æ’å…¥æ€§èƒ½...")
    start_time = time.time()
    conn.execute("CREATE OR REPLACE TABLE large_test_table AS SELECT * FROM large_df")
    insert_time = time.time() - start_time
    
    print(f"æ’å…¥ 100,000 è¡Œæ•°æ®ç”¨æ—¶: {insert_time:.4f} ç§’, é€Ÿåº¦: {int(100000/insert_time)} è¡Œ/ç§’")
    
    # æµ‹è¯•å¤æ‚æŸ¥è¯¢æ€§èƒ½
    print("\næµ‹è¯•å¤æ‚æŸ¥è¯¢æ€§èƒ½...")
    
    # èšåˆæŸ¥è¯¢
    start_time = time.time()
    agg_result = conn.execute("""
        SELECT 
            category,
            COUNT(*) as order_count,
            SUM(price * quantity) as total_revenue,
            AVG(price) as avg_price
        FROM large_test_table
        GROUP BY category
        ORDER BY total_revenue DESC
    """).df()
    agg_time = time.time() - start_time
    
    print(f"å¤æ‚èšåˆæŸ¥è¯¢ç”¨æ—¶: {agg_time:.4f} ç§’")
    print("èšåˆç»“æœé¢„è§ˆ:")
    print(agg_result)
    
    # è¿æ¥æŸ¥è¯¢
    start_time = time.time()
    join_result = conn.execute("""
        SELECT 
            ltt.category,
            COUNT(DISTINCT ltt.user_id) as unique_users,
            AVG(ltt.price) as avg_price
        FROM large_test_table ltt
        GROUP BY ltt.category
        HAVING COUNT(DISTINCT ltt.user_id) > 100
        ORDER BY unique_users DESC
    """).df()
    join_time = time.time() - start_time
    
    print(f"è¿æ¥æŸ¥è¯¢ç”¨æ—¶: {join_time:.4f} ç§’")
    print("è¿æ¥æŸ¥è¯¢ç»“æœé¢„è§ˆ:")
    print(join_result.head())
    
    # æµ‹è¯•æ›´æ–°æ€§èƒ½
    print("\næµ‹è¯•æ›´æ–°æ€§èƒ½...")
    start_time = time.time()
    conn.execute("""
        UPDATE large_test_table
        SET price = price * 1.1
        WHERE category = 'Electronics'
    """)
    update_time = time.time() - start_time
    
    print(f"æ›´æ–°ç”µå­äº§å“ä»·æ ¼ç”¨æ—¶: {update_time:.4f} ç§’")
    
    # æµ‹è¯•åˆ é™¤æ€§èƒ½
    print("\næµ‹è¯•åˆ é™¤æ€§èƒ½...")
    start_time = time.time()
    conn.execute("""
        DELETE FROM large_test_table
        WHERE date < '2023-06-01'
    """)
    delete_time = time.time() - start_time
    
    remaining_count = conn.execute("SELECT COUNT(*) FROM large_test_table").fetchone()[0]
    print(f"åˆ é™¤å†å²æ•°æ®ç”¨æ—¶: {delete_time:.4f} ç§’, å‰©ä½™è®°å½•æ•°: {remaining_count}")
    
    # å…³é—­è¿æ¥
    conn.close()
    
    return {
        'insert_time': insert_time,
        'agg_query_time': agg_time,
        'join_query_time': join_time,
        'update_time': update_time,
        'delete_time': delete_time
    }

def performance_comparison_test():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯• - DuckDB vs Pandas"""
    print("\nâš–ï¸ DuckDB vs Pandas æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    
    import random
    from datetime import datetime, timedelta
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ç”Ÿæˆæµ‹è¯•æ•°æ®é›† (50,000 è¡Œ)...")
    test_data = {
        "user_id": [f"user_{random.randint(1, 1000)}" for _ in range(50000)],
        "category": [random.choice(['A', 'B', 'C', 'D', 'E']) for _ in range(50000)],
        "value": [random.uniform(1, 100) for _ in range(50000)],
        "quantity": [random.randint(1, 10) for _ in range(50000)]
    }
    
    df = pd.DataFrame(test_data)
    
    # DuckDB æµ‹è¯•
    print("ä½¿ç”¨ DuckDB è¿›è¡Œèšåˆæ“ä½œ...")
    conn = duckdb.connect()
    conn.execute("CREATE TABLE test_data AS SELECT * FROM df")
    
    start_time = time.time()
    duckdb_result = conn.execute("""
        SELECT 
            category,
            COUNT(*) as count,
            AVG(value) as avg_value,
            SUM(value * quantity) as total_value
        FROM test_data
        GROUP BY category
        ORDER BY total_value DESC
    """).df()
    duckdb_time = time.time() - start_time
    
    print(f"DuckDB èšåˆç”¨æ—¶: {duckdb_time:.4f} ç§’")
    
    # Pandas æµ‹è¯•
    print("ä½¿ç”¨ Pandas è¿›è¡Œç›¸åŒèšåˆæ“ä½œ...")
    start_time = time.time()
    pandas_result = df.groupby('category').agg({
        'user_id': 'count',
        'value': 'mean',
        'quantity': lambda x: (df.loc[x.index, 'value'] * x).sum()
    }).reset_index()
    pandas_result.columns = ['category', 'count', 'avg_value', 'total_value']
    pandas_result = pandas_result.sort_values('total_value', ascending=False)
    pandas_time = time.time() - start_time
    
    print(f"Pandas èšåˆç”¨æ—¶: {pandas_time:.4f} ç§’")
    
    # æ€§èƒ½å¯¹æ¯”
    speedup = pandas_time / duckdb_time if duckdb_time > 0 else float('inf')
    print(f"DuckDB æ¯” Pandas å¿« {speedup:.2f} å€")
    
    # æ˜¾ç¤ºç»“æœ
    print("\nDuckDB ç»“æœ:")
    print(duckdb_result)
    print("\nPandas ç»“æœ:")
    print(pandas_result)
    
    # å…³é—­è¿æ¥
    conn.close()
    
    return {
        'duckdb_time': duckdb_time,
        'pandas_time': pandas_time,
        'speedup': speedup
    }

def performance_full_test():
    """å®Œæ•´æ€§èƒ½æµ‹è¯•"""
    print("\nğŸš€ å®Œæ•´æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ‰§è¡Œå„é¡¹æ€§èƒ½æµ‹è¯•
    print("å¼€å§‹å¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•...")
    large_test_results = performance_large_dataset_test()
    
    print("\nå¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
    comparison_results = performance_comparison_test()
    
    # æ±‡æ€»ç»“æœ
    print("\nğŸ“‹ æ€§èƒ½æµ‹è¯•æ±‡æ€»:")
    print(f"å¤§æ•°æ®é›†æ’å…¥æ€§èƒ½: {large_test_results['insert_time']:.4f} ç§’")
    print(f"å¤æ‚èšåˆæŸ¥è¯¢æ€§èƒ½: {large_test_results['agg_query_time']:.4f} ç§’")
    print(f"è¿æ¥æŸ¥è¯¢æ€§èƒ½: {large_test_results['join_query_time']:.4f} ç§’")
    print(f"æ›´æ–°æ“ä½œæ€§èƒ½: {large_test_results['update_time']:.4f} ç§’")
    print(f"åˆ é™¤æ“ä½œæ€§èƒ½: {large_test_results['delete_time']:.4f} ç§’")
    print(f"DuckDB vs Pandas é€Ÿåº¦æå‡: {comparison_results['speedup']:.2f} å€")
    
    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    performance_full_test()
"""
æ•°æ®å¤„ç†æ¨¡å—
æ¼”ç¤ºä»æ–‡ä»¶åŠ è½½æ•°æ®ã€æ‰¹é‡å†™å…¥ç­‰æ“ä½œ
"""

import duckdb
import pandas as pd
import os

def load_from_csv(conn, csv_path):
    """ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®åˆ°è¡¨"""
    print(f"ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return False
    
    # ä»CSVåˆ›å»ºæˆ–æ›¿æ¢è¡¨
    conn.execute(f"""
        CREATE OR REPLACE TABLE sales_from_csv AS 
        SELECT * FROM read_csv_auto('{csv_path}', header=True)
    """)
    
    # è·å–è®°å½•æ•°
    count = conn.execute("SELECT COUNT(*) FROM sales_from_csv").fetchone()[0]
    print(f"æˆåŠŸåŠ è½½ {count} æ¡è®°å½•")
    return True

def load_from_parquet(conn, parquet_path):
    """ä»Parquetæ–‡ä»¶åŠ è½½æ•°æ®åˆ°è¡¨"""
    print(f"ä»Parquetæ–‡ä»¶åŠ è½½æ•°æ®: {parquet_path}")
    
    if not os.path.exists(parquet_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {parquet_path}")
        return False
    
    conn.execute(f"""
        CREATE OR REPLACE TABLE sales_from_parquet AS 
        SELECT * FROM read_parquet('{parquet_path}')
    """)
    
    count = conn.execute("SELECT COUNT(*) FROM sales_from_parquet").fetchone()[0]
    print(f"æˆåŠŸåŠ è½½ {count} æ¡è®°å½•")
    return True

def generate_large_dataset(rows=100000):
    """ç”Ÿæˆå¤§æ•°æ®é›†ç”¨äºæ€§èƒ½æµ‹è¯•"""
    print(f"ç”ŸæˆåŒ…å« {rows} è¡Œçš„å¤§æ•°æ®é›†...")
    
    import random
    from datetime import datetime, timedelta
    import pandas as pd
    
    # ç”Ÿæˆéšæœºæ•°æ®
    categories = ['Electronics', 'Clothing', 'Home', 'Food', 'Books']
    products = ['Laptop', 'Phone', 'Shirt', 'Desk', 'Apple', 'Novel', 'Headphones', 'Watch', 'Chair', 'Cookbook']
    regions = ['North', 'South', 'East', 'West', 'Central']
    
    data = {
        "user_id": [f"user_{random.randint(1, 1000)}" for _ in range(rows)],
        "product": [random.choice(products) for _ in range(rows)],
        "category": [random.choice(categories) for _ in range(rows)],
        "price": [round(random.uniform(10, 1000), 2) for _ in range(rows)],
        "quantity": [random.randint(1, 10) for _ in range(rows)],
        "region": [random.choice(regions) for _ in range(rows)],
        "date": [(datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d') for _ in range(rows)]
    }
    
    df = pd.DataFrame(data)
    return df

def bulk_insert_from_dataframe(conn, df, table_name="large_sales"):
    """ä»DataFrameæ‰¹é‡æ’å…¥æ•°æ®"""
    print(f"ä»DataFrameæ‰¹é‡æ’å…¥æ•°æ®åˆ°è¡¨ {table_name}...")
    
    # åˆ›å»ºè¡¨å¹¶æ’å…¥æ•°æ®
    conn.execute(f"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df")
    
    count = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
    print(f"æˆåŠŸæ’å…¥ {count} æ¡è®°å½•åˆ°è¡¨ {table_name}")
    return True

def create_sample_csv():
    """åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶"""
    sample_data = {
        "order_id": [101, 102, 103, 104, 105],
        "product": ["Tablet", "Jacket", "Blender", "Magazine", "Smartphone"],
        "category": ["Electronics", "Clothing", "Home", "Education", "Electronics"],
        "region": ["North", "South", "East", "West", "Central"],
        "sales_amount": [300.00, 85.00, 120.00, 15.00, 650.00],
        "date": ["2023-02-01", "2023-02-02", "2023-02-03", "2023-02-04", "2023-02-05"]
    }
    
    df = pd.DataFrame(sample_data)
    df.to_csv("data/sample_sales.csv", index=False)
    print("ç¤ºä¾‹CSVæ–‡ä»¶å·²åˆ›å»º: data/sample_sales.csv")

def demo_data_processing():
    """æ¼”ç¤ºæ•°æ®å¤„ç†åŠŸèƒ½"""
    print("\nğŸš€ æ•°æ®å¤„ç†æ¼”ç¤º")
    print("=" * 50)
    
    # è¿æ¥åˆ°æ•°æ®åº“
    conn = duckdb.connect("hello_duckdb.duckdb")
    
    # åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶
    create_sample_csv()
    
    # ä»CSVåŠ è½½æ•°æ®
    load_from_csv(conn, "data/sample_sales.csv")
    
    # æ˜¾ç¤ºä»CSVåŠ è½½çš„æ•°æ®
    print("\nä»CSVåŠ è½½çš„æ•°æ®:")
    result = conn.execute("SELECT * FROM sales_from_csv").df()
    print(result.head())
    
    # ç”Ÿæˆå¹¶æ’å…¥å¤§æ•°æ®é›†
    large_df = generate_large_dataset(10000)  # ç”Ÿæˆ1ä¸‡æ¡è®°å½•ä»¥èŠ‚çœæ—¶é—´
    bulk_insert_from_dataframe(conn, large_df, "demo_large_sales")
    
    # æ˜¾ç¤ºå¤§æ•°æ®é›†çš„éƒ¨åˆ†ä¿¡æ¯
    print(f"\nå¤§æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    stats = conn.execute("""
        SELECT 
            COUNT(*) as total_records,
            AVG(price) as avg_price,
            MIN(price) as min_price,
            MAX(price) as max_price,
            SUM(price * quantity) as total_revenue
        FROM demo_large_sales
    """).df()
    print(stats)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
    print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
    category_stats = conn.execute("""
        SELECT 
            category,
            COUNT(*) as record_count,
            AVG(price) as avg_price,
            SUM(price * quantity) as total_revenue
        FROM demo_large_sales
        GROUP BY category
        ORDER BY total_revenue DESC
    """).df()
    print(category_stats)
    
    # å…³é—­è¿æ¥
    conn.close()
    print("\nâœ… æ•°æ®å¤„ç†æ¼”ç¤ºå®Œæˆ")

if __name__ == "__main__":
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    os.makedirs("data", exist_ok=True)
    demo_data_processing()